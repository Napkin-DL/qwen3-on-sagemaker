{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Fine-tune Qwen3 on Amazon SageMaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%store -r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_model_id : Qwen/Qwen3-4B\n",
      "bucket : sagemaker-us-west-2-322537213286\n",
      "model_weight_path : s3://sagemaker-us-west-2-322537213286/checkpoints/qwen3-4b\n",
      "training_input_path : s3://sagemaker-us-west-2-322537213286/korean-openthoughts-114k-normalized/train/train_dataset.json\n",
      "local_training_input_path : /home/ec2-user/SageMaker/TRAINING/qwen3-on-sagemaker/dataset/train\n",
      "registered_model : qwen3-4b\n"
     ]
    }
   ],
   "source": [
    "print(f\"test_model_id : {test_model_id}\")\n",
    "print(f\"bucket : {bucket}\")\n",
    "print(f\"model_weight_path : {model_weight_path}\")\n",
    "print(f\"training_input_path : {training_input_path}\")\n",
    "# print(f\"test_input_path : {test_input_path}\")\n",
    "print(f\"local_training_input_path : {local_training_input_path}\")\n",
    "# print(f\"local_test_input_path : {local_test_input_path}\")\n",
    "print(f\"registered_model : {registered_model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/pydantic/_internal/_fields.py:172: UserWarning: Field name \"json\" in \"MonitoringDatasetFormat\" shadows an attribute in parent \"Base\"\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[05/05/25 09:44:00] </span><span style=\"color: #0069ff; text-decoration-color: #0069ff; font-weight: bold\">INFO    </span> Found credentials from IAM Role:                                   <a href=\"file:///home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/botocore/credentials.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">credentials.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/botocore/credentials.py#1132\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1132</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         BaseNotebookInstanceEc2InstanceRole                                <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                   </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[05/05/25 09:44:00]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;0;105;255mINFO    \u001b[0m Found credentials from IAM Role:                                   \u001b]8;id=924007;file:///home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/botocore/credentials.py\u001b\\\u001b[2mcredentials.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=751819;file:///home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/botocore/credentials.py#1132\u001b\\\u001b[2m1132\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         BaseNotebookInstanceEc2InstanceRole                                \u001b[2m                   \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #0069ff; text-decoration-color: #0069ff; font-weight: bold\">INFO    </span> Found credentials from IAM Role:                                   <a href=\"file:///home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/botocore/credentials.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">credentials.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/botocore/credentials.py#1132\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1132</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         BaseNotebookInstanceEc2InstanceRole                                <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                   </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;0;105;255mINFO    \u001b[0m Found credentials from IAM Role:                                   \u001b]8;id=189522;file:///home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/botocore/credentials.py\u001b\\\u001b[2mcredentials.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=432117;file:///home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/botocore/credentials.py#1132\u001b\\\u001b[2m1132\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         BaseNotebookInstanceEc2InstanceRole                                \u001b[2m                   \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #0069ff; text-decoration-color: #0069ff; font-weight: bold\">INFO    </span> Found credentials from IAM Role:                                   <a href=\"file:///home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/botocore/credentials.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">credentials.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/botocore/credentials.py#1132\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1132</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         BaseNotebookInstanceEc2InstanceRole                                <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                   </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;0;105;255mINFO    \u001b[0m Found credentials from IAM Role:                                   \u001b]8;id=922759;file:///home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/botocore/credentials.py\u001b\\\u001b[2mcredentials.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=436893;file:///home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/botocore/credentials.py#1132\u001b\\\u001b[2m1132\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         BaseNotebookInstanceEc2InstanceRole                                \u001b[2m                   \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sagemaker\n",
    "from pathlib import Path\n",
    "from time import strftime\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.243.3'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sagemaker.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 모델 fine-tuning을 위한 파라미터 설정\n",
    "\n",
    "이제 모델을 fine-tuning할 준비가 되었습니다. `trl`의 [SFTTrainer](https://huggingface.co/docs/trl/sft_trainer)를 사용하여 모델을 fine-tuning하겠습니다. SFTTrainer는 오픈 LLM을 지도 학습 방식으로 fine-tuning하는 것을 간소화합니다. SFTTrainer는 `transformers`의 `Trainer` 클래스의 하위 클래스입니다. 데이터셋을 디스크에서 로드하고, 모델과 토크나이저를 준비하고 훈련을 시작하는 스크립트 [sm_qlora_trainer.py](./src/sm_qlora_trainer.py)를 준비했습니다. 이 스크립트는 `trl`의 [SFTTrainer](https://huggingface.co/docs/trl/sft_trainer)를 사용하여 모델을 fine-tuning하며 다음 기능을 지원합니다:\n",
    "`yaml` 파일은 데이터셋과 유사하게 Amazon SageMaker에 업로드되고 제공됩니다. 이 설정 파일을 `qwen3-4b.yaml`로 저장하고 S3에 업로드합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mkdir -p src/configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/configs/qwen3-4b.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/configs/qwen3-4b.yaml\n",
    "# 스크립트 기본 매개변수\n",
    "model_name_or_path: \"/opt/ml/input/data/model_weight\"\n",
    "train_dataset_path: \"/opt/ml/input/data/training\"\n",
    "output_dir: \"/opt/ml/checkpoints\"\n",
    "tokenizers_parallelism: \"false\"\n",
    "\n",
    "# 모델 설정 - 메모리 최적화\n",
    "model:\n",
    "  load_in_4bit: true\n",
    "  bnb_4bit_use_double_quant: true  # 이중 양자화 활성화\n",
    "  bnb_4bit_quant_type: \"nf4\"\n",
    "  use_bf16: false  # fp16 사용 (메모리 효율적)\n",
    "  trust_remote_code: true\n",
    "  low_cpu_mem_usage: true\n",
    "  use_cache: false  # 캐시 비활성화로 메모리 절약\n",
    "  offload_folder: \"offload\"  # 디스크 오프로딩 설정\n",
    "  offload_state_dict: true  # 상태 딕셔너리 오프로딩\n",
    "\n",
    "# 토크나이저 설정\n",
    "tokenizer:\n",
    "  trust_remote_code: true\n",
    "  use_fast: true\n",
    "  padding_side: \"right\"\n",
    "\n",
    "# LoRA 설정 - 메모리 최적화\n",
    "lora:\n",
    "  lora_alpha: 16\n",
    "  lora_dropout: 0.05\n",
    "  lora_r: 64  # r 값 감소로 메모리 사용량 감소\n",
    "  bias: \"none\"\n",
    "  target_modules:\n",
    "    - \"q_proj\"\n",
    "    - \"k_proj\"\n",
    "    - \"v_proj\"\n",
    "    - \"o_proj\"\n",
    "    - \"gate_proj\"\n",
    "    - \"up_proj\"\n",
    "    - \"down_proj\"\n",
    "\n",
    "# 데이터 설정 - 메모리 최적화\n",
    "data:\n",
    "  train_path: \"train_dataset.json\"\n",
    "  text_column: \"text\"\n",
    "  max_seq_length: 2048\n",
    "  padding: false  # 동적 패딩 사용\n",
    "  truncation: true\n",
    "\n",
    "# 데이터셋 처리 설정 - 메모리 최적화\n",
    "dataset:\n",
    "  preprocessing_batch_size: 50  # 작은 배치 크기로 처리\n",
    "  num_proc: 1\n",
    "  streaming: false  # 필요시 true로 설정하여 스트리밍 활성화\n",
    "\n",
    "# 데이터 콜레이터 설정\n",
    "data_collator:\n",
    "  mlm: false\n",
    "  pad_to_multiple_of: 8\n",
    "\n",
    "# 학습 설정 - 메모리 최적화\n",
    "training:\n",
    "  per_device_train_batch_size: 1  # 배치 크기 감소\n",
    "  gradient_accumulation_steps: 8  # 증가하여 효과적인 배치 크기 유지\n",
    "  learning_rate: 2.0e-3\n",
    "  num_train_epochs: 5\n",
    "  logging_steps: 10\n",
    "  warmup_steps: 10\n",
    "  optim: \"adamw_torch_fused\"  # 최적화된 옵티마이저\n",
    "  group_by_length: true  # 길이별 그룹화로 패딩 최소화\n",
    "  save_strategy: \"steps\"\n",
    "  save_steps: 500\n",
    "  save_total_limit: 1  # 저장 모델 수 감소\n",
    "  seed: 42\n",
    "  dataloader_num_workers: 0  # 워커 수 감소\n",
    "  report_to: \"none\"  # 보고 비활성화\n",
    "  ddp_find_unused_parameters: false\n",
    "  gradient_checkpointing: true  # 그래디언트 체크포인팅 활성화\n",
    "  max_grad_norm: 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from sagemaker.huggingface import HuggingFace\n",
    "# import torch\n",
    "\n",
    "training_hyperparameters={}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create SageMaker Training Job\n",
    "\n",
    "SageMaker 학습 작업을 생성하기 위해서는 `HuggingFace` Estimator가 필요합니다. Estimator는 Amazon SageMaker의 end-to-end 학습 및 배포 작업을 처리합니다. Estimator는 인프라 사용을 관리합니다. Amazon SageMaker는 필요한 모든 ec2 인스턴스를 시작하고 관리하며, 적절한 huggingface 컨테이너를 제공하고, 제공된 스크립트를 업로드하고 S3 버킷의 데이터를 컨테이너의 `/opt/ml/input/data`로 다운로드합니다. 그런 다음 학습 작업을 시작합니다.\n",
    "\n",
    "> Note: 사용자 정의 학습 스크립트를 사용하는 경우 `source_dir`에 `requirements.txt`를 포함해야 합니다. 전체 리포지토리를 클론하는 것을 권장합니다.\n",
    "\n",
    "스크립트 실행에 `torchrun`을 사용하려면 Estimator에서 `distribution` 파라미터를 정의하고 `{\"torch_distributed\": {\"enabled\": True}}`로 설정하기만 하면 됩니다. 이렇게 하면 SageMaker가 다음과 같이 학습 작업을 실행합니다:\n",
    "\n",
    "```python\n",
    "torchrun --nnodes 2 --nproc_per_node 8 --master_addr algo-1 --master_port 7777 --node_rank 1 sm_qlora_trainer.py --config /opt/ml/input/data/config/config.yaml\n",
    "```\n",
    "아래의 HuggingFace 설정은 1x A10 GPU가 있는 1x ml.g5.2xlarge에서 학습 작업을 시작합니다. SageMaker의 놀라운 점은 instance_count를 수정하여 쉽게 ml.p4d.24xlarge 또는 2x ml.p4d.24xlarge로 확장할 수 있다는 것입니다. SageMaker가 나머지를 처리해줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "instance_type = 'ml.g5.2xlarge'\n",
    "# instance_type = 'ml.p4d.24xlarge'\n",
    "# instance_type = 'ml.p5.48xlarge'\n",
    "# instance_type = 'local_gpu'\n",
    "instance_count = 1\n",
    "max_run = 72*60*60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ec2-user/SageMaker/TRAINING/qwen3-on-sagemaker/qwen3-4b'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "local_model_weight_path = f\"{Path.cwd()}/{registered_model}\"\n",
    "local_model_weight_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[05/05/25 09:44:01] </span><span style=\"color: #0069ff; text-decoration-color: #0069ff; font-weight: bold\">INFO    </span> Found credentials from IAM Role:                                   <a href=\"file:///home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/botocore/credentials.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">credentials.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/botocore/credentials.py#1132\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1132</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         BaseNotebookInstanceEc2InstanceRole                                <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                   </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[05/05/25 09:44:01]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;0;105;255mINFO    \u001b[0m Found credentials from IAM Role:                                   \u001b]8;id=30380;file:///home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/botocore/credentials.py\u001b\\\u001b[2mcredentials.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=172392;file:///home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/botocore/credentials.py#1132\u001b\\\u001b[2m1132\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         BaseNotebookInstanceEc2InstanceRole                                \u001b[2m                   \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('s3://sagemaker-us-west-2-322537213286/korean-openthoughts-114k-normalized/train/train_dataset.json',\n",
       " 's3://sagemaker-us-west-2-322537213286/checkpoints/qwen3-4b')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if instance_type =='local_gpu':\n",
    "    import os\n",
    "    from sagemaker.local import LocalSession\n",
    "\n",
    "    sagemaker_session = LocalSession()\n",
    "    sagemaker_session.config = {'local': {'local_code': True}}\n",
    "    training = f\"file://{local_training_input_path}\"\n",
    "    # test = f\"file://{local_test_input_path}\"\n",
    "    model_weight = f\"file://{local_model_weight_path}\"\n",
    "else:\n",
    "    sagemaker_session = sagemaker.Session()\n",
    "    training = training_input_path\n",
    "    # test = test_input_path\n",
    "    model_weight = model_weight_path\n",
    "\n",
    "training, model_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #0069ff; text-decoration-color: #0069ff; font-weight: bold\">INFO    </span> Found credentials from IAM Role:                                   <a href=\"file:///home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/botocore/credentials.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">credentials.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/botocore/credentials.py#1132\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1132</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         BaseNotebookInstanceEc2InstanceRole                                <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                   </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;0;105;255mINFO    \u001b[0m Found credentials from IAM Role:                                   \u001b]8;id=808254;file:///home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/botocore/credentials.py\u001b\\\u001b[2mcredentials.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=328994;file:///home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/botocore/credentials.py#1132\u001b\\\u001b[2m1132\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         BaseNotebookInstanceEc2InstanceRole                                \u001b[2m                   \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "import time\n",
    "# define Training Job Name \n",
    "job_name = f'huggingface-{registered_model}-{time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.localtime())}'\n",
    "\n",
    "# distribution={ \"pytorchddp\": { \"enabled\": True } }  # mpirun, activates SMDDP AllReduce OR AllGather\n",
    "# distribution={\"mpi\": {\"enabled\": True}}\n",
    "distribution={\n",
    "    \"torch_distributed\": {\n",
    "        \"enabled\": True,\n",
    "        # \"NCCL_DEBUG\":\"INFO\"\n",
    "        # \"mpi\": \"-verbose -x NCCL_DEBUG=INFO\"\n",
    "    }\n",
    "}  # torchrun, activates SMDDP AllGather\n",
    "# distribution={ \"smdistributed\": { \"dataparallel\": { \"enabled\": True } } }  # mpirun, activates SMDDP AllReduce OR AllGather\n",
    "\n",
    "environment={\n",
    "    \"NCCL_DEBUG\" : \"INFO\", \n",
    "    \"SM_LOG_LEVEL\": \"10\",\n",
    "}\n",
    "\n",
    "training_hyperparameters[\"config\"] = \"/opt/ml/code/configs/qwen3-4b.yaml\"\n",
    "    \n",
    "estimator = PyTorch(\n",
    "                    entry_point='sm_lora_trainer.py',\n",
    "                    source_dir=f'{Path.cwd()}/src',\n",
    "                    role=role,\n",
    "                    # image_uri=image_uri,\n",
    "                    framework_version='2.3.0',\n",
    "                    py_version='py311',\n",
    "                    instance_count=instance_count,\n",
    "                    instance_type=instance_type,\n",
    "                    distribution=distribution,\n",
    "                    disable_profiler=True,\n",
    "                    debugger_hook_config=False,\n",
    "                    max_run=max_run,\n",
    "                    hyperparameters=training_hyperparameters,\n",
    "                    sagemaker_session=sagemaker_session,\n",
    "                    # enable_remote_debug=True,\n",
    "                    # keep_alive_period_in_seconds=1200,\n",
    "                    # input_mode='FastFile'\n",
    "                    # max_wait=max_run,\n",
    "                    # use_spot_instances=True,\n",
    "                    # subnets=['subnet-090e278f3622051c4'],\n",
    "                    # security_group_ids=['sg-05baa06337a188842'],\n",
    "                    max_retry_attempts=30,\n",
    "                    environment=environment,\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!sudo rm -rf src/core.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #0069ff; text-decoration-color: #0069ff; font-weight: bold\">INFO    </span> SageMaker Python SDK will collect telemetry to help us better  <a href=\"file:///home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/sagemaker/telemetry/telemetry_logging.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">telemetry_logging.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/sagemaker/telemetry/telemetry_logging.py#91\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">91</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         understand our user's needs, diagnose issues, and deliver      <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                       </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         additional features.                                           <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                       </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         To opt out of telemetry, please disable via TelemetryOptOut    <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                       </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         parameter in SDK defaults config. For more information, refer  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                       </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         to                                                             <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                       </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #0069ff; text-decoration-color: #0069ff; text-decoration: underline\">https://sagemaker.readthedocs.io/en/stable/overview.html#confi</span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                       </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #0069ff; text-decoration-color: #0069ff; text-decoration: underline\">guring-and-using-defaults-with-the-sagemaker-python-sdk.</span>       <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                       </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;0;105;255mINFO    \u001b[0m SageMaker Python SDK will collect telemetry to help us better  \u001b]8;id=272963;file:///home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/sagemaker/telemetry/telemetry_logging.py\u001b\\\u001b[2mtelemetry_logging.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=156410;file:///home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/sagemaker/telemetry/telemetry_logging.py#91\u001b\\\u001b[2m91\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         understand our user's needs, diagnose issues, and deliver      \u001b[2m                       \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         additional features.                                           \u001b[2m                       \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         To opt out of telemetry, please disable via TelemetryOptOut    \u001b[2m                       \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         parameter in SDK defaults config. For more information, refer  \u001b[2m                       \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         to                                                             \u001b[2m                       \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[4;38;2;0;105;255mhttps://sagemaker.readthedocs.io/en/stable/overview.html#confi\u001b[0m \u001b[2m                       \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[4;38;2;0;105;255mguring-and-using-defaults-with-the-sagemaker-python-sdk.\u001b[0m       \u001b[2m                       \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[05/05/25 09:44:02] </span><span style=\"color: #0069ff; text-decoration-color: #0069ff; font-weight: bold\">INFO    </span> image_uri is not presented, retrieving image_uri based on            <a href=\"file:///home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/sagemaker/image_uris.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">image_uris.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/sagemaker/image_uris.py#681\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">681</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         instance_type, framework etc.                                        <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                 </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[05/05/25 09:44:02]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;0;105;255mINFO    \u001b[0m image_uri is not presented, retrieving image_uri based on            \u001b]8;id=403348;file:///home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/sagemaker/image_uris.py\u001b\\\u001b[2mimage_uris.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=567929;file:///home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/sagemaker/image_uris.py#681\u001b\\\u001b[2m681\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         instance_type, framework etc.                                        \u001b[2m                 \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #0069ff; text-decoration-color: #0069ff; font-weight: bold\">INFO    </span> Creating training-job with name:                                       <a href=\"file:///home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/sagemaker/session.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">session.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/sagemaker/session.py#1042\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1042</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         qwen3-4b-ml-g5-2xlarge-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0505</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">09441746438241</span>                           <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;0;105;255mINFO    \u001b[0m Creating training-job with name:                                       \u001b]8;id=871257;file:///home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/sagemaker/session.py\u001b\\\u001b[2msession.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=435907;file:///home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/sagemaker/session.py#1042\u001b\\\u001b[2m1042\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         qwen3-4b-ml-g5-2xlarge-\u001b[1;36m1\u001b[0m-\u001b[1;36m0505\u001b[0m-\u001b[1;36m09441746438241\u001b[0m                           \u001b[2m               \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "current_time = strftime(\"%m%d-%H%M%s\")\n",
    "i_type = instance_type.replace('.','-')\n",
    "job_name = f'{registered_model}-{i_type}-{instance_count}-{current_time}'\n",
    "\n",
    "\n",
    "if instance_type =='local_gpu':\n",
    "    estimator.checkpoint_s3_uri = None\n",
    "else:\n",
    "    estimator.checkpoint_s3_uri = f's3://{bucket}/checkpoint/{test_model_id}/{job_name}'\n",
    "    \n",
    "    \n",
    "estimator.fit(\n",
    "    inputs={\n",
    "        'training': training,\n",
    "        'model_weight' : model_weight\n",
    "    }, \n",
    "    job_name=job_name,\n",
    "    wait=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-05 09:44:02 Starting - Starting the training job\n",
      "2025-05-05 09:44:02 Pending - Training job waiting for capacity...............\n",
      "2025-05-05 09:46:28 Pending - Preparing the instances for training...\n",
      "2025-05-05 09:46:56 Downloading - Downloading input data.........\n",
      "2025-05-05 09:48:11 Downloading - Downloading the training image............\n",
      "2025-05-05 09:50:17 Training - Training image download completed. Training in progress..\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34mCUDA compat package should be installed for NVIDIA driver smaller than 530.30.02\u001b[0m\n",
      "\u001b[34mCurrent installed NVIDIA driver version is 550.163.01\u001b[0m\n",
      "\u001b[34mSkipping CUDA compat setup as newer NVIDIA driver is installed\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.11/site-packages/paramiko/pkey.py:100: CryptographyDeprecationWarning: TripleDES has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.TripleDES and will be removed from cryptography.hazmat.primitives.ciphers.algorithms in 48.0.0.\n",
      "  \"cipher\": algorithms.TripleDES,\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.11/site-packages/paramiko/transport.py:259: CryptographyDeprecationWarning: TripleDES has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.TripleDES and will be removed from cryptography.hazmat.primitives.ciphers.algorithms in 48.0.0.\n",
      "  \"class\": algorithms.TripleDES,\u001b[0m\n",
      "\u001b[34m2025-05-05 09:50:39,954 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2025-05-05 09:50:39,971 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2025-05-05 09:50:39,980 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2025-05-05 09:50:39,981 sagemaker_pytorch_container.training INFO     Invoking TorchDistributed...\u001b[0m\n",
      "\u001b[34m2025-05-05 09:50:39,981 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2025-05-05 09:50:41,226 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt\u001b[0m\n",
      "\u001b[34mCollecting wandb==0.19.10 (from -r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading wandb-0.19.10-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\u001b[0m\n",
      "\u001b[34mCollecting datasets==3.5.1 (from -r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading datasets-3.5.1-py3-none-any.whl.metadata (19 kB)\u001b[0m\n",
      "\u001b[34mCollecting peft==0.15.2 (from -r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mDownloading peft-0.15.2-py3-none-any.whl.metadata (13 kB)\u001b[0m\n",
      "\u001b[34mCollecting trl==0.17.0 (from -r requirements.txt (line 4))\u001b[0m\n",
      "\u001b[34mDownloading trl-0.17.0-py3-none-any.whl.metadata (12 kB)\u001b[0m\n",
      "\u001b[34mCollecting bitsandbytes==0.45.5 (from -r requirements.txt (line 5))\u001b[0m\n",
      "\u001b[34mDownloading bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl.metadata (5.0 kB)\u001b[0m\n",
      "\u001b[34mCollecting tokenizers==0.21.1 (from -r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[34mDownloading tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\u001b[0m\n",
      "\u001b[34mCollecting transformers==4.51.3 (from -r requirements.txt (line 7))\u001b[0m\n",
      "\u001b[34mDownloading transformers-4.51.3-py3-none-any.whl.metadata (38 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: click!=8.0.0,>=7.1 in /opt/conda/lib/python3.11/site-packages (from wandb==0.19.10->-r requirements.txt (line 1)) (8.1.7)\u001b[0m\n",
      "\u001b[34mCollecting docker-pycreds>=0.4.0 (from wandb==0.19.10->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading docker_pycreds-0.4.0-py2.py3-none-any.whl.metadata (1.8 kB)\u001b[0m\n",
      "\u001b[34mCollecting gitpython!=3.1.29,>=1.0.0 (from wandb==0.19.10->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading GitPython-3.1.44-py3-none-any.whl.metadata (13 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: platformdirs in /opt/conda/lib/python3.11/site-packages (from wandb==0.19.10->-r requirements.txt (line 1)) (4.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /opt/conda/lib/python3.11/site-packages (from wandb==0.19.10->-r requirements.txt (line 1)) (3.20.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.11/site-packages (from wandb==0.19.10->-r requirements.txt (line 1)) (5.9.8)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pydantic<3 in /opt/conda/lib/python3.11/site-packages (from wandb==0.19.10->-r requirements.txt (line 1)) (2.7.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml in /opt/conda/lib/python3.11/site-packages (from wandb==0.19.10->-r requirements.txt (line 1)) (6.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests<3,>=2.0.0 in /opt/conda/lib/python3.11/site-packages (from wandb==0.19.10->-r requirements.txt (line 1)) (2.32.2)\u001b[0m\n",
      "\u001b[34mCollecting sentry-sdk>=2.0.0 (from wandb==0.19.10->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading sentry_sdk-2.27.0-py2.py3-none-any.whl.metadata (10 kB)\u001b[0m\n",
      "\u001b[34mCollecting setproctitle (from wandb==0.19.10->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading setproctitle-1.3.6-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: setuptools in /opt/conda/lib/python3.11/site-packages (from wandb==0.19.10->-r requirements.txt (line 1)) (78.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions<5,>=4.4 in /opt/conda/lib/python3.11/site-packages (from wandb==0.19.10->-r requirements.txt (line 1)) (4.11.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from datasets==3.5.1->-r requirements.txt (line 2)) (3.14.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.11/site-packages (from datasets==3.5.1->-r requirements.txt (line 2)) (1.26.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.11/site-packages (from datasets==3.5.1->-r requirements.txt (line 2)) (19.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.11/site-packages (from datasets==3.5.1->-r requirements.txt (line 2)) (0.3.8)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /opt/conda/lib/python3.11/site-packages (from datasets==3.5.1->-r requirements.txt (line 2)) (2.2.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.66.3 in /opt/conda/lib/python3.11/site-packages (from datasets==3.5.1->-r requirements.txt (line 2)) (4.66.4)\u001b[0m\n",
      "\u001b[34mCollecting xxhash (from datasets==3.5.1->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multiprocess<0.70.17 in /opt/conda/lib/python3.11/site-packages (from datasets==3.5.1->-r requirements.txt (line 2)) (0.70.16)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /opt/conda/lib/python3.11/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.5.1->-r requirements.txt (line 2)) (2024.5.0)\u001b[0m\n",
      "\u001b[34mCollecting aiohttp (from datasets==3.5.1->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading aiohttp-3.11.18-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\u001b[0m\n",
      "\u001b[34mCollecting huggingface-hub>=0.24.0 (from datasets==3.5.1->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading huggingface_hub-0.30.2-py3-none-any.whl.metadata (13 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging in /opt/conda/lib/python3.11/site-packages (from datasets==3.5.1->-r requirements.txt (line 2)) (23.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.11/site-packages (from peft==0.15.2->-r requirements.txt (line 3)) (2.3.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: accelerate>=0.21.0 in /opt/conda/lib/python3.11/site-packages (from peft==0.15.2->-r requirements.txt (line 3)) (0.30.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: safetensors in /opt/conda/lib/python3.11/site-packages (from peft==0.15.2->-r requirements.txt (line 3)) (0.4.3)\u001b[0m\n",
      "\u001b[34mCollecting accelerate>=0.21.0 (from peft==0.15.2->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mDownloading accelerate-1.6.0-py3-none-any.whl.metadata (19 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: rich in /opt/conda/lib/python3.11/site-packages (from trl==0.17.0->-r requirements.txt (line 4)) (13.7.1)\u001b[0m\n",
      "\u001b[34mCollecting regex!=2019.12.17 (from transformers==4.51.3->-r requirements.txt (line 7))\u001b[0m\n",
      "\u001b[34mDownloading regex-2024.11.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>=1.4.0 in /opt/conda/lib/python3.11/site-packages (from docker-pycreds>=0.4.0->wandb==0.19.10->-r requirements.txt (line 1)) (1.16.0)\u001b[0m\n",
      "\u001b[34mCollecting aiohappyeyeballs>=2.3.0 (from aiohttp->datasets==3.5.1->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\u001b[0m\n",
      "\u001b[34mCollecting aiosignal>=1.1.2 (from aiohttp->datasets==3.5.1->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets==3.5.1->-r requirements.txt (line 2)) (23.2.0)\u001b[0m\n",
      "\u001b[34mCollecting frozenlist>=1.1.1 (from aiohttp->datasets==3.5.1->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading frozenlist-1.6.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\u001b[0m\n",
      "\u001b[34mCollecting multidict<7.0,>=4.5 (from aiohttp->datasets==3.5.1->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading multidict-6.4.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\u001b[0m\n",
      "\u001b[34mCollecting propcache>=0.2.0 (from aiohttp->datasets==3.5.1->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading propcache-0.3.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\u001b[0m\n",
      "\u001b[34mCollecting yarl<2.0,>=1.17.0 (from aiohttp->datasets==3.5.1->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading yarl-1.20.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (72 kB)\u001b[0m\n",
      "\u001b[34mCollecting gitdb<5,>=4.0.1 (from gitpython!=3.1.29,>=1.0.0->wandb==0.19.10->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading gitdb-4.0.12-py3-none-any.whl.metadata (1.2 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.11/site-packages (from pydantic<3->wandb==0.19.10->-r requirements.txt (line 1)) (0.7.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pydantic-core==2.18.2 in /opt/conda/lib/python3.11/site-packages (from pydantic<3->wandb==0.19.10->-r requirements.txt (line 1)) (2.18.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.0.0->wandb==0.19.10->-r requirements.txt (line 1)) (3.3.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.0.0->wandb==0.19.10->-r requirements.txt (line 1)) (3.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.0.0->wandb==0.19.10->-r requirements.txt (line 1)) (1.26.20)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.0.0->wandb==0.19.10->-r requirements.txt (line 1)) (2025.1.31)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sympy in /opt/conda/lib/python3.11/site-packages (from torch>=1.13.0->peft==0.15.2->-r requirements.txt (line 3)) (1.12)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (from torch>=1.13.0->peft==0.15.2->-r requirements.txt (line 3)) (3.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch>=1.13.0->peft==0.15.2->-r requirements.txt (line 3)) (3.1.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.11/site-packages (from pandas->datasets==3.5.1->-r requirements.txt (line 2)) (2.9.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas->datasets==3.5.1->-r requirements.txt (line 2)) (2024.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.11/site-packages (from pandas->datasets==3.5.1->-r requirements.txt (line 2)) (2024.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.11/site-packages (from rich->trl==0.17.0->-r requirements.txt (line 4)) (3.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.11/site-packages (from rich->trl==0.17.0->-r requirements.txt (line 4)) (2.18.0)\u001b[0m\n",
      "\u001b[34mCollecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb==0.19.10->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading smmap-5.0.2-py3-none-any.whl.metadata (4.3 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich->trl==0.17.0->-r requirements.txt (line 4)) (0.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch>=1.13.0->peft==0.15.2->-r requirements.txt (line 3)) (2.1.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.11/site-packages (from sympy->torch>=1.13.0->peft==0.15.2->-r requirements.txt (line 3)) (1.3.0)\u001b[0m\n",
      "\u001b[34mDownloading wandb-0.19.10-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (21.3 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 21.3/21.3 MB 137.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading datasets-3.5.1-py3-none-any.whl (491 kB)\u001b[0m\n",
      "\u001b[34mDownloading peft-0.15.2-py3-none-any.whl (411 kB)\u001b[0m\n",
      "\u001b[34mDownloading trl-0.17.0-py3-none-any.whl (348 kB)\u001b[0m\n",
      "\u001b[34mDownloading bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl (76.1 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.1/76.1 MB 163.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.0/3.0 MB 148.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading transformers-4.51.3-py3-none-any.whl (10.4 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 10.4/10.4 MB 156.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading accelerate-1.6.0-py3-none-any.whl (354 kB)\u001b[0m\n",
      "\u001b[34mDownloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\u001b[0m\n",
      "\u001b[34mDownloading aiohttp-3.11.18-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.7/1.7 MB 119.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading GitPython-3.1.44-py3-none-any.whl (207 kB)\u001b[0m\n",
      "\u001b[34mDownloading huggingface_hub-0.30.2-py3-none-any.whl (481 kB)\u001b[0m\n",
      "\u001b[34mDownloading regex-2024.11.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (792 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 792.7/792.7 kB 82.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading sentry_sdk-2.27.0-py2.py3-none-any.whl (340 kB)\u001b[0m\n",
      "\u001b[34mDownloading setproctitle-1.3.6-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (31 kB)\u001b[0m\n",
      "\u001b[34mDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\u001b[0m\n",
      "\u001b[34mDownloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\u001b[0m\n",
      "\u001b[34mDownloading aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\u001b[0m\n",
      "\u001b[34mDownloading frozenlist-1.6.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (313 kB)\u001b[0m\n",
      "\u001b[34mDownloading gitdb-4.0.12-py3-none-any.whl (62 kB)\u001b[0m\n",
      "\u001b[34mDownloading multidict-6.4.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (223 kB)\u001b[0m\n",
      "\u001b[34mDownloading propcache-0.3.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (232 kB)\u001b[0m\n",
      "\u001b[34mDownloading yarl-1.20.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (358 kB)\u001b[0m\n",
      "\u001b[34mDownloading smmap-5.0.2-py3-none-any.whl (24 kB)\u001b[0m\n",
      "\u001b[34mInstalling collected packages: xxhash, smmap, setproctitle, sentry-sdk, regex, propcache, multidict, frozenlist, docker-pycreds, aiohappyeyeballs, yarl, huggingface-hub, gitdb, aiosignal, tokenizers, gitpython, bitsandbytes, aiohttp, accelerate, wandb, transformers, peft, datasets, trl\u001b[0m\n",
      "\u001b[34mAttempting uninstall: huggingface-hub\u001b[0m\n",
      "\u001b[34mFound existing installation: huggingface_hub 0.23.0\u001b[0m\n",
      "\u001b[34mUninstalling huggingface_hub-0.23.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled huggingface_hub-0.23.0\u001b[0m\n",
      "\u001b[34mAttempting uninstall: accelerate\u001b[0m\n",
      "\u001b[34mFound existing installation: accelerate 0.30.1\u001b[0m\n",
      "\u001b[34mUninstalling accelerate-0.30.1:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled accelerate-0.30.1\u001b[0m\n",
      "\u001b[34mSuccessfully installed accelerate-1.6.0 aiohappyeyeballs-2.6.1 aiohttp-3.11.18 aiosignal-1.3.2 bitsandbytes-0.45.5 datasets-3.5.1 docker-pycreds-0.4.0 frozenlist-1.6.0 gitdb-4.0.12 gitpython-3.1.44 huggingface-hub-0.30.2 multidict-6.4.3 peft-0.15.2 propcache-0.3.1 regex-2024.11.6 sentry-sdk-2.27.0 setproctitle-1.3.6 smmap-5.0.2 tokenizers-0.21.1 transformers-4.51.3 trl-0.17.0 wandb-0.19.10 xxhash-3.5.0 yarl-1.20.0\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\n",
      "\u001b[34m[notice] A new release of pip is available: 25.0.1 -> 25.1.1\u001b[0m\n",
      "\u001b[34m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m2025-05-05 09:50:54,554 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2025-05-05 09:50:54,554 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2025-05-05 09:50:54,595 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2025-05-05 09:50:54,674 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2025-05-05 09:50:54,683 sagemaker-training-toolkit INFO     Starting distributed training through torchrun\u001b[0m\n",
      "\u001b[34m2025-05-05 09:50:54,701 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2025-05-05 09:50:54,710 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {\n",
      "        \"sagemaker_instance_type\": \"ml.g5.2xlarge\",\n",
      "        \"sagemaker_torch_distributed_enabled\": true\n",
      "    },\n",
      "    \"channel_input_dirs\": {\n",
      "        \"model_weight\": \"/opt/ml/input/data/model_weight\",\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.g5.2xlarge\",\n",
      "    \"distribution_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"distribution_instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"config\": \"/opt/ml/code/configs/qwen3-4b.yaml\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"model_weight\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.g5.2xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": false,\n",
      "    \"is_smddprun_installed\": true,\n",
      "    \"job_name\": \"qwen3-4b-ml-g5-2xlarge-1-0505-09441746438241\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-west-2-322537213286/qwen3-4b-ml-g5-2xlarge-1-0505-09441746438241/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"sm_lora_trainer\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 8,\n",
      "    \"num_gpus\": 1,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.g5.2xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.g5.2xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"sm_lora_trainer.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"config\":\"/opt/ml/code/configs/qwen3-4b.yaml\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=sm_lora_trainer.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={\"sagemaker_instance_type\":\"ml.g5.2xlarge\",\"sagemaker_torch_distributed_enabled\":true}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.2xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.2xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"model_weight\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"model_weight\",\"training\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.g5.2xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.2xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=sm_lora_trainer\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=8\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-west-2-322537213286/qwen3-4b-ml-g5-2xlarge-1-0505-09441746438241/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{\"sagemaker_instance_type\":\"ml.g5.2xlarge\",\"sagemaker_torch_distributed_enabled\":true},\"channel_input_dirs\":{\"model_weight\":\"/opt/ml/input/data/model_weight\",\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.g5.2xlarge\",\"distribution_hosts\":[\"algo-1\"],\"distribution_instance_groups\":[\"homogeneousCluster\"],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"config\":\"/opt/ml/code/configs/qwen3-4b.yaml\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"model_weight\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.2xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":false,\"is_smddprun_installed\":true,\"job_name\":\"qwen3-4b-ml-g5-2xlarge-1-0505-09441746438241\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-west-2-322537213286/qwen3-4b-ml-g5-2xlarge-1-0505-09441746438241/source/sourcedir.tar.gz\",\"module_name\":\"sm_lora_trainer\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":1,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.2xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.2xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"sm_lora_trainer.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--config\",\"/opt/ml/code/configs/qwen3-4b.yaml\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_MODEL_WEIGHT=/opt/ml/input/data/model_weight\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_HP_CONFIG=/opt/ml/code/configs/qwen3-4b.yaml\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python311.zip:/opt/conda/lib/python3.11:/opt/conda/lib/python3.11/lib-dynload:/opt/conda/lib/python3.11/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34mtorchrun --nnodes 1 --nproc_per_node 1 sm_lora_trainer.py --config /opt/ml/code/configs/qwen3-4b.yaml\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  33%|███▎      | 1/3 [00:01<00:02,  1.18s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  67%|██████▋   | 2/3 [00:02<00:01,  1.28s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 3/3 [00:02<00:00,  1.17it/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 0 examples [00:00, ? examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 209 examples [00:00, 3360.81 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/209 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 209/209 [00:04<00:00, 47.79 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 209/209 [00:04<00:00, 47.73 examples/s]\u001b[0m\n",
      "\u001b[34mINFO:__main__:LoRA 설정: r=64, alpha=16, dropout=0.05\u001b[0m\n",
      "\u001b[34mINFO:__main__:LoRA 타겟 모듈: {'up_proj', 'q_proj', 'o_proj', 'k_proj', 'gate_proj', 'down_proj', 'v_proj'}\u001b[0m\n",
      "\u001b[34mtrainable params: 132,120,576 || all params: 4,154,588,672 || trainable%: 3.1801\u001b[0m\n",
      "\u001b[34mTruncating train dataset:   0%|          | 0/209 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mTruncating train dataset: 100%|██████████| 209/209 [00:00<00:00, 39410.58 examples/s]\u001b[0m\n",
      "\u001b[34mNCCL version 2.21.5+cuda12.1\u001b[0m\n",
      "\u001b[34malgo-1:84:121 [0] nccl_net_ofi_create_plugin:204 NCCL WARN NET/OFI Failed to initialize sendrecv protocol\u001b[0m\n",
      "\u001b[34malgo-1:84:121 [0] nccl_net_ofi_create_plugin:257 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\u001b[0m\n",
      "\u001b[34mINFO:__main__:학습 시작...\u001b[0m\n",
      "\u001b[34m0%|          | 0/130 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m1%|          | 1/130 [00:05<12:39,  5.89s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 2/130 [00:11<12:32,  5.88s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 3/130 [00:17<12:26,  5.88s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 4/130 [00:23<12:09,  5.79s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 5/130 [00:28<11:58,  5.75s/it]\u001b[0m\n",
      "\u001b[34m5%|▍         | 6/130 [00:34<11:49,  5.72s/it]\u001b[0m\n",
      "\u001b[34m5%|▌         | 7/130 [00:40<11:41,  5.70s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 8/130 [00:45<11:35,  5.70s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 9/130 [00:51<11:36,  5.76s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 10/130 [00:57<11:27,  5.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4267, 'grad_norm': 0.11368252336978912, 'learning_rate': 0.0018000000000000002, 'num_tokens': 80992.0, 'mean_token_accuracy': 0.6817047588527203, 'epoch': 0.38}\u001b[0m\n",
      "\u001b[34m8%|▊         | 10/130 [00:57<11:27,  5.73s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 11/130 [01:03<11:20,  5.72s/it]\u001b[0m\n",
      "\u001b[34m9%|▉         | 12/130 [01:09<11:21,  5.77s/it]\u001b[0m\n",
      "\u001b[34m10%|█         | 13/130 [01:14<11:11,  5.74s/it]\u001b[0m\n",
      "\u001b[34m11%|█         | 14/130 [01:20<11:11,  5.79s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 15/130 [01:26<11:02,  5.76s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 16/130 [01:32<10:53,  5.73s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 17/130 [01:37<10:45,  5.72s/it]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 18/130 [01:43<10:39,  5.71s/it]\u001b[0m\n",
      "\u001b[34m15%|█▍        | 19/130 [01:49<10:39,  5.76s/it]\u001b[0m\n",
      "\u001b[34m15%|█▌        | 20/130 [01:55<10:31,  5.74s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.95, 'grad_norm': 0.10769078135490417, 'learning_rate': 0.00185, 'num_tokens': 162397.0, 'mean_token_accuracy': 0.7661358088254928, 'epoch': 0.77}\u001b[0m\n",
      "\u001b[34m15%|█▌        | 20/130 [01:55<10:31,  5.74s/it]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 21/130 [02:00<10:30,  5.78s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 22/130 [02:06<10:21,  5.76s/it]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 23/130 [02:12<10:13,  5.73s/it]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 24/130 [02:17<10:05,  5.71s/it]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 25/130 [02:23<10:06,  5.77s/it]\u001b[0m\n",
      "\u001b[34m20%|██        | 26/130 [02:29<09:57,  5.74s/it]\u001b[0m\n",
      "\u001b[34m21%|██        | 27/130 [02:30<07:16,  4.24s/it]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 28/130 [02:35<07:56,  4.67s/it]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 29/130 [02:41<08:22,  4.97s/it]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 30/130 [02:47<08:38,  5.18s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.8129, 'grad_norm': 0.10509154945611954, 'learning_rate': 0.0016833333333333333, 'num_tokens': 236342.0, 'mean_token_accuracy': 0.7874283864073557, 'epoch': 1.11}\u001b[0m\n",
      "\u001b[34m23%|██▎       | 30/130 [02:47<08:38,  5.18s/it]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 31/130 [02:52<08:48,  5.34s/it]\u001b[0m\n",
      "\u001b[34m25%|██▍       | 32/130 [02:58<08:52,  5.43s/it]\u001b[0m\n",
      "\u001b[34m25%|██▌       | 33/130 [03:04<09:00,  5.58s/it]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 34/130 [03:10<08:57,  5.60s/it]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 35/130 [03:15<08:54,  5.63s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 36/130 [03:21<08:56,  5.71s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 37/130 [03:27<08:50,  5.70s/it]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 38/130 [03:33<08:50,  5.76s/it]\u001b[0m\n",
      "\u001b[34m30%|███       | 39/130 [03:39<08:48,  5.80s/it]\u001b[0m\n",
      "\u001b[34m31%|███       | 40/130 [03:44<08:39,  5.77s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.732, 'grad_norm': 0.11742912232875824, 'learning_rate': 0.0015166666666666666, 'num_tokens': 317316.0, 'mean_token_accuracy': 0.8098168134689331, 'epoch': 1.5}\u001b[0m\n",
      "\u001b[34m31%|███       | 40/130 [03:44<08:39,  5.77s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 41/130 [03:50<08:31,  5.74s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 42/130 [03:56<08:23,  5.72s/it]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 43/130 [04:02<08:16,  5.71s/it]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 44/130 [04:07<08:10,  5.70s/it]\u001b[0m\n",
      "\u001b[34m35%|███▍      | 45/130 [04:13<08:03,  5.69s/it]\u001b[0m\n",
      "\u001b[34m35%|███▌      | 46/130 [04:19<08:08,  5.81s/it]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 47/130 [04:25<07:58,  5.76s/it]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 48/130 [04:30<07:55,  5.80s/it]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 49/130 [04:36<07:46,  5.76s/it]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 50/130 [04:42<07:38,  5.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.6841, 'grad_norm': 0.11333674937486649, 'learning_rate': 0.00135, 'num_tokens': 398127.0, 'mean_token_accuracy': 0.8187029153108597, 'epoch': 1.88}\u001b[0m\n",
      "\u001b[34m38%|███▊      | 50/130 [04:42<07:38,  5.73s/it]\u001b[0m\n",
      "\u001b[34m39%|███▉      | 51/130 [04:48<07:36,  5.78s/it]\u001b[0m\n",
      "\u001b[34m40%|████      | 52/130 [04:53<07:27,  5.74s/it]\u001b[0m\n",
      "\u001b[34m41%|████      | 53/130 [04:59<07:20,  5.72s/it]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 54/130 [05:00<05:20,  4.22s/it]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 55/130 [05:05<05:49,  4.66s/it]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 56/130 [05:11<06:06,  4.96s/it]\u001b[0m\n",
      "\u001b[34m44%|████▍     | 57/130 [05:17<06:21,  5.23s/it]\u001b[0m\n",
      "\u001b[34m45%|████▍     | 58/130 [05:23<06:26,  5.36s/it]\u001b[0m\n",
      "\u001b[34m45%|████▌     | 59/130 [05:28<06:27,  5.45s/it]\u001b[0m\n",
      "\u001b[34m46%|████▌     | 60/130 [05:35<06:40,  5.71s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.5493, 'grad_norm': 0.14201973378658295, 'learning_rate': 0.0011833333333333333, 'num_tokens': 471580.0, 'mean_token_accuracy': 0.8547988142052741, 'epoch': 2.23}\u001b[0m\n",
      "\u001b[34m46%|████▌     | 60/130 [05:35<06:40,  5.71s/it]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 61/130 [05:40<06:33,  5.70s/it]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 62/130 [05:46<06:26,  5.69s/it]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 63/130 [05:52<06:20,  5.68s/it]\u001b[0m\n",
      "\u001b[34m49%|████▉     | 64/130 [05:57<06:14,  5.68s/it]\u001b[0m\n",
      "\u001b[34m50%|█████     | 65/130 [06:03<06:12,  5.74s/it]\u001b[0m\n",
      "\u001b[34m51%|█████     | 66/130 [06:09<06:05,  5.71s/it]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 67/130 [06:15<06:03,  5.76s/it]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 68/130 [06:21<06:03,  5.86s/it]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 69/130 [06:26<05:53,  5.80s/it]\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 70/130 [06:32<05:45,  5.76s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.4704, 'grad_norm': 0.16317586600780487, 'learning_rate': 0.0010166666666666666, 'num_tokens': 552354.0, 'mean_token_accuracy': 0.8712208636105061, 'epoch': 2.61}\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 70/130 [06:32<05:45,  5.76s/it]\u001b[0m\n",
      "\u001b[34m55%|█████▍    | 71/130 [06:38<05:38,  5.74s/it]\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 72/130 [06:43<05:31,  5.71s/it]\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 73/130 [06:49<05:24,  5.70s/it]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 74/130 [06:55<05:18,  5.69s/it]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 75/130 [07:00<05:12,  5.68s/it]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 76/130 [07:06<05:06,  5.67s/it]\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 77/130 [07:12<04:58,  5.62s/it]\u001b[0m\n",
      "\u001b[34m60%|██████    | 78/130 [07:17<04:50,  5.59s/it]\u001b[0m\n",
      "\u001b[34m61%|██████    | 79/130 [07:23<04:44,  5.57s/it]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 80/130 [07:28<04:37,  5.56s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.4694, 'grad_norm': 0.13471798598766327, 'learning_rate': 0.00085, 'num_tokens': 634274.0, 'mean_token_accuracy': 0.871114369481802, 'epoch': 3.0}\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 80/130 [07:28<04:37,  5.56s/it]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 81/130 [07:29<03:21,  4.11s/it]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 82/130 [07:34<03:37,  4.53s/it]\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 83/130 [07:40<03:46,  4.83s/it]\u001b[0m\n",
      "\u001b[34m65%|██████▍   | 84/130 [07:45<03:51,  5.04s/it]\u001b[0m\n",
      "\u001b[34m65%|██████▌   | 85/130 [07:51<03:53,  5.18s/it]\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 86/130 [07:57<03:55,  5.35s/it]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 87/130 [08:02<03:52,  5.40s/it]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 88/130 [08:08<03:48,  5.44s/it]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 89/130 [08:13<03:44,  5.46s/it]\u001b[0m\n",
      "\u001b[34m69%|██████▉   | 90/130 [08:19<03:39,  5.48s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.3096, 'grad_norm': 0.12712235748767853, 'learning_rate': 0.0006833333333333333, 'num_tokens': 708466.0, 'mean_token_accuracy': 0.9193350141995573, 'epoch': 3.34}\u001b[0m\n",
      "\u001b[34m69%|██████▉   | 90/130 [08:19<03:39,  5.48s/it]\u001b[0m\n",
      "\u001b[34m70%|███████   | 91/130 [08:24<03:34,  5.49s/it]\u001b[0m\n",
      "\u001b[34m71%|███████   | 92/130 [08:30<03:31,  5.57s/it]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 93/130 [08:36<03:25,  5.56s/it]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 94/130 [08:41<03:22,  5.61s/it]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 95/130 [08:47<03:15,  5.59s/it]\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 96/130 [08:52<03:09,  5.57s/it]\u001b[0m\n",
      "\u001b[34m75%|███████▍  | 97/130 [08:58<03:03,  5.56s/it]\u001b[0m\n",
      "\u001b[34m75%|███████▌  | 98/130 [09:03<02:57,  5.55s/it]\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 99/130 [09:09<02:53,  5.61s/it]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 100/130 [09:15<02:47,  5.58s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.2631, 'grad_norm': 0.15083225071430206, 'learning_rate': 0.0005166666666666667, 'num_tokens': 789820.0, 'mean_token_accuracy': 0.9259138010442257, 'epoch': 3.73}\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 100/130 [09:15<02:47,  5.58s/it]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 101/130 [09:21<02:43,  5.63s/it]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 102/130 [09:26<02:38,  5.67s/it]\u001b[0m\n",
      "\u001b[34m79%|███████▉  | 103/130 [09:32<02:31,  5.63s/it]\u001b[0m\n",
      "\u001b[34m80%|████████  | 104/130 [09:38<02:27,  5.66s/it]\u001b[0m\n",
      "\u001b[34m81%|████████  | 105/130 [09:43<02:20,  5.62s/it]\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 106/130 [09:49<02:14,  5.60s/it]\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 107/130 [09:54<02:09,  5.64s/it]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 108/130 [09:55<01:31,  4.16s/it]\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 109/130 [10:01<01:37,  4.64s/it]\u001b[0m\n",
      "\u001b[34m85%|████████▍ | 110/130 [10:07<01:40,  5.04s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.2247, 'grad_norm': 0.13113777339458466, 'learning_rate': 0.00035, 'num_tokens': 862930.0, 'mean_token_accuracy': 0.9383708655017696, 'epoch': 4.08}\u001b[0m\n",
      "\u001b[34m85%|████████▍ | 110/130 [10:07<01:40,  5.04s/it]\u001b[0m\n",
      "\u001b[34m85%|████████▌ | 111/130 [10:12<01:38,  5.19s/it]\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 112/130 [10:18<01:35,  5.29s/it]\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 113/130 [10:23<01:31,  5.36s/it]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 114/130 [10:29<01:26,  5.41s/it]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 115/130 [10:34<01:21,  5.44s/it]\u001b[0m\n",
      "\u001b[34m89%|████████▉ | 116/130 [10:40<01:16,  5.47s/it]\u001b[0m\n",
      "\u001b[34m90%|█████████ | 117/130 [10:45<01:11,  5.48s/it]\u001b[0m\n",
      "\u001b[34m91%|█████████ | 118/130 [10:51<01:05,  5.50s/it]\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 119/130 [10:57<01:01,  5.57s/it]\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 120/130 [11:02<00:55,  5.56s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.1332, 'grad_norm': 0.12288225442171097, 'learning_rate': 0.00018333333333333334, 'num_tokens': 944655.0, 'mean_token_accuracy': 0.966210225969553, 'epoch': 4.46}\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 120/130 [11:02<00:55,  5.56s/it]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 121/130 [11:08<00:49,  5.55s/it]\u001b[0m\n",
      "\u001b[34m94%|█████████▍| 122/130 [11:13<00:44,  5.54s/it]\u001b[0m\n",
      "\u001b[34m95%|█████████▍| 123/130 [11:19<00:38,  5.53s/it]\u001b[0m\n",
      "\u001b[34m95%|█████████▌| 124/130 [11:24<00:33,  5.53s/it]\u001b[0m\n",
      "\u001b[34m96%|█████████▌| 125/130 [11:30<00:27,  5.53s/it]\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 126/130 [11:35<00:22,  5.53s/it]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 127/130 [11:41<00:16,  5.53s/it]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 128/130 [11:47<00:11,  5.66s/it]\u001b[0m\n",
      "\u001b[34m99%|█████████▉| 129/130 [11:53<00:05,  5.69s/it]\u001b[0m\n",
      "\u001b[34m100%|██████████| 130/130 [11:58<00:00,  5.64s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.1252, 'grad_norm': 0.10950285196304321, 'learning_rate': 1.6666666666666667e-05, 'num_tokens': 1025340.0, 'mean_token_accuracy': 0.9687342345714569, 'epoch': 4.84}\u001b[0m\n",
      "\u001b[34m100%|██████████| 130/130 [11:58<00:00,  5.64s/it]\u001b[0m\n",
      "\u001b[34m{'train_runtime': 721.1819, 'train_samples_per_second': 1.449, 'train_steps_per_second': 0.18, 'train_loss': 0.5500382588459896, 'epoch': 4.84}\u001b[0m\n",
      "\u001b[34m100%|██████████| 130/130 [12:01<00:00,  5.64s/it]\u001b[0m\n",
      "\u001b[34m100%|██████████| 130/130 [12:01<00:00,  5.55s/it]\u001b[0m\n",
      "\u001b[34mINFO:__main__:모델 저장 중...\u001b[0m\n",
      "\u001b[34mINFO:__main__:모델이 /opt/ml/checkpoints에 저장되었습니다.\u001b[0m\n",
      "\u001b[34m2025-05-05 10:03:21,119 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2025-05-05 10:03:21,119 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2025-05-05 10:03:21,120 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2025-05-05 10:03:34 Uploading - Uploading generated training model\n",
      "2025-05-05 10:03:34 Completed - Training job completed\n",
      "Training seconds: 998\n",
      "Billable seconds: 998\n"
     ]
    }
   ],
   "source": [
    "sagemaker_session = sagemaker.Session()\n",
    "sagemaker_session.logs_for_job(job_name=job_name, wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PEFT 모델 추론 하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "sagemaker_session = sagemaker.Session()\n",
    "train_result = sagemaker_session.describe_training_job(job_name=job_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-us-west-2-322537213286/checkpoint/Qwen/Qwen3-4B/qwen3-4b-ml-g5-2xlarge-1-0505-09441746438241'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint_s3uri = train_result['CheckpointConfig']['S3Uri']\n",
    "checkpoint_s3uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           PRE checkpoint-130/\n",
      "2025-05-05 10:03:16       5105 README.md\n",
      "2025-05-05 10:03:17        864 adapter_config.json\n",
      "2025-05-05 10:03:17  528550256 adapter_model.safetensors\n",
      "2025-05-05 10:03:17        707 added_tokens.json\n",
      "2025-05-05 10:03:17    1671853 merges.txt\n",
      "2025-05-05 10:03:17        613 special_tokens_map.json\n",
      "2025-05-05 10:03:17   11422934 tokenizer.json\n",
      "2025-05-05 10:03:17       9706 tokenizer_config.json\n",
      "2025-05-05 10:03:17    2776833 vocab.json\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls $checkpoint_s3uri/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "output_dir = './checkpoints'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://sagemaker-us-west-2-322537213286/checkpoint/Qwen/Qwen3-4B/qwen3-4b-ml-g5-2xlarge-1-0505-09441746438241/README.md to checkpoints/README.md\n",
      "download: s3://sagemaker-us-west-2-322537213286/checkpoint/Qwen/Qwen3-4B/qwen3-4b-ml-g5-2xlarge-1-0505-09441746438241/checkpoint-130/README.md to checkpoints/checkpoint-130/README.md\n",
      "download: s3://sagemaker-us-west-2-322537213286/checkpoint/Qwen/Qwen3-4B/qwen3-4b-ml-g5-2xlarge-1-0505-09441746438241/adapter_config.json to checkpoints/adapter_config.json\n",
      "download: s3://sagemaker-us-west-2-322537213286/checkpoint/Qwen/Qwen3-4B/qwen3-4b-ml-g5-2xlarge-1-0505-09441746438241/checkpoint-130/adapter_config.json to checkpoints/checkpoint-130/adapter_config.json\n",
      "download: s3://sagemaker-us-west-2-322537213286/checkpoint/Qwen/Qwen3-4B/qwen3-4b-ml-g5-2xlarge-1-0505-09441746438241/checkpoint-130/added_tokens.json to checkpoints/checkpoint-130/added_tokens.json\n",
      "download: s3://sagemaker-us-west-2-322537213286/checkpoint/Qwen/Qwen3-4B/qwen3-4b-ml-g5-2xlarge-1-0505-09441746438241/added_tokens.json to checkpoints/added_tokens.json\n",
      "download: s3://sagemaker-us-west-2-322537213286/checkpoint/Qwen/Qwen3-4B/qwen3-4b-ml-g5-2xlarge-1-0505-09441746438241/checkpoint-130/rng_state.pth to checkpoints/checkpoint-130/rng_state.pth\n",
      "download: s3://sagemaker-us-west-2-322537213286/checkpoint/Qwen/Qwen3-4B/qwen3-4b-ml-g5-2xlarge-1-0505-09441746438241/checkpoint-130/merges.txt to checkpoints/checkpoint-130/merges.txt\n",
      "download: s3://sagemaker-us-west-2-322537213286/checkpoint/Qwen/Qwen3-4B/qwen3-4b-ml-g5-2xlarge-1-0505-09441746438241/checkpoint-130/scaler.pt to checkpoints/checkpoint-130/scaler.pt\n",
      "download: s3://sagemaker-us-west-2-322537213286/checkpoint/Qwen/Qwen3-4B/qwen3-4b-ml-g5-2xlarge-1-0505-09441746438241/checkpoint-130/scheduler.pt to checkpoints/checkpoint-130/scheduler.pt\n",
      "download: s3://sagemaker-us-west-2-322537213286/checkpoint/Qwen/Qwen3-4B/qwen3-4b-ml-g5-2xlarge-1-0505-09441746438241/checkpoint-130/special_tokens_map.json to checkpoints/checkpoint-130/special_tokens_map.json\n",
      "download: s3://sagemaker-us-west-2-322537213286/checkpoint/Qwen/Qwen3-4B/qwen3-4b-ml-g5-2xlarge-1-0505-09441746438241/checkpoint-130/tokenizer_config.json to checkpoints/checkpoint-130/tokenizer_config.json\n",
      "download: s3://sagemaker-us-west-2-322537213286/checkpoint/Qwen/Qwen3-4B/qwen3-4b-ml-g5-2xlarge-1-0505-09441746438241/checkpoint-130/trainer_state.json to checkpoints/checkpoint-130/trainer_state.json\n",
      "download: s3://sagemaker-us-west-2-322537213286/checkpoint/Qwen/Qwen3-4B/qwen3-4b-ml-g5-2xlarge-1-0505-09441746438241/checkpoint-130/tokenizer.json to checkpoints/checkpoint-130/tokenizer.json\n",
      "download: s3://sagemaker-us-west-2-322537213286/checkpoint/Qwen/Qwen3-4B/qwen3-4b-ml-g5-2xlarge-1-0505-09441746438241/checkpoint-130/training_args.bin to checkpoints/checkpoint-130/training_args.bin\n",
      "download: s3://sagemaker-us-west-2-322537213286/checkpoint/Qwen/Qwen3-4B/qwen3-4b-ml-g5-2xlarge-1-0505-09441746438241/merges.txt to checkpoints/merges.txt\n",
      "download: s3://sagemaker-us-west-2-322537213286/checkpoint/Qwen/Qwen3-4B/qwen3-4b-ml-g5-2xlarge-1-0505-09441746438241/checkpoint-130/vocab.json to checkpoints/checkpoint-130/vocab.json\n",
      "download: s3://sagemaker-us-west-2-322537213286/checkpoint/Qwen/Qwen3-4B/qwen3-4b-ml-g5-2xlarge-1-0505-09441746438241/special_tokens_map.json to checkpoints/special_tokens_map.json\n",
      "download: s3://sagemaker-us-west-2-322537213286/checkpoint/Qwen/Qwen3-4B/qwen3-4b-ml-g5-2xlarge-1-0505-09441746438241/tokenizer_config.json to checkpoints/tokenizer_config.json\n",
      "download: s3://sagemaker-us-west-2-322537213286/checkpoint/Qwen/Qwen3-4B/qwen3-4b-ml-g5-2xlarge-1-0505-09441746438241/tokenizer.json to checkpoints/tokenizer.json\n",
      "download: s3://sagemaker-us-west-2-322537213286/checkpoint/Qwen/Qwen3-4B/qwen3-4b-ml-g5-2xlarge-1-0505-09441746438241/vocab.json to checkpoints/vocab.json\n",
      "download: s3://sagemaker-us-west-2-322537213286/checkpoint/Qwen/Qwen3-4B/qwen3-4b-ml-g5-2xlarge-1-0505-09441746438241/adapter_model.safetensors to checkpoints/adapter_model.safetensors\n",
      "download: s3://sagemaker-us-west-2-322537213286/checkpoint/Qwen/Qwen3-4B/qwen3-4b-ml-g5-2xlarge-1-0505-09441746438241/checkpoint-130/adapter_model.safetensors to checkpoints/checkpoint-130/adapter_model.safetensors\n",
      "download: s3://sagemaker-us-west-2-322537213286/checkpoint/Qwen/Qwen3-4B/qwen3-4b-ml-g5-2xlarge-1-0505-09441746438241/checkpoint-130/optimizer.pt to checkpoints/checkpoint-130/optimizer.pt\n"
     ]
    }
   ],
   "source": [
    "!rm -rf $output_dir\n",
    "!aws s3 sync $checkpoint_s3uri $output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!rm -rf $output_dir/checkpoint-*\n",
    "!rm -rf $output_dir/compressed_model\n",
    "!rm -rf $output_dir/runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "local_model_weight_path=f'{Path.cwd()}/{registered_model}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b08d988e8ea24023b3f397753381725d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from peft import PeftModel, PeftConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import torch\n",
    "peft_model_id = output_dir\n",
    "\n",
    "# Reload model in FP16 and merge it with LoRA weights\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    local_model_weight_path,\n",
    "    low_cpu_mem_usage=True,\n",
    "    return_dict=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "peft_model = PeftModel.from_pretrained(base_model, peft_model_id)\n",
    "peft_model = peft_model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "merged_save_dir = \"merged_model\"\n",
    "peft_model.save_pretrained(merged_save_dir, safe_serialization=True, max_shard_size=\"2GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('merged_model/tokenizer_config.json',\n",
       " 'merged_model/special_tokens_map.json',\n",
       " 'merged_model/vocab.json',\n",
       " 'merged_model/merges.txt',\n",
       " 'merged_model/added_tokens.json',\n",
       " 'merged_model/tokenizer.json')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reload tokenizer to save it\n",
    "tokenizer = AutoTokenizer.from_pretrained(local_model_weight_path, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "tokenizer.save_pretrained(merged_save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "device = torch.cuda.current_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_prompt_style = \"\"\"너는 reasoning, analysis, problem-solving에 advanced knowledge를 갖춘 AI Assistant입니다.\n",
    "    <question> 질문에 가장 적절한 답변을 작성하세요. 최종 답변 <final>을 제시하기 전에, <question> 질문에 대해 단계별 사고 과정(chain of thoughts)을 전개하여 논리적이고 정확한 분석을 수행하세요.\n",
    "    \n",
    "    <question>\n",
    "    {}\n",
    "    </question>\n",
    "    ### 주의사항:\n",
    "    - 불필요한 인사말이나 서두, input은 생략하고, 바로 <response> 부터 작성해주세요.\n",
    "    - 질문과 답변을 반복하지 마세요\n",
    "    - 단계별 사고 과정은 충분히 상세하게 작성하되, 최종 답변은 간결하게 정리하세요\n",
    "    \n",
    "\n",
    "    \n",
    "    ### 응답 형식:\n",
    "    <think>\n",
    "        ### THINKING\n",
    "        [여기에 한국어로 단계별 사고 과정을 상세히 기술하세요. 문제를 분석하고, 가능한 접근법을 검토하며, 논리적 추론을 통해 결론에 도달하는 과정을 보여주세요.]\n",
    "    </think>\n",
    "    <final>\n",
    "        ### FINAL-ANSWER\n",
    "        [THINKING에서 도출된 결론을 간결하고 명확하게 요약하여 한국어로 최종 답변으로 제시하세요.]\n",
    "    </final>\n",
    "\n",
    "    아래 답변입니다.\n",
    "    <think>\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "너는 reasoning, analysis, problem-solving에 advanced knowledge를 갖춘 AI Assistant입니다.\n",
      "    <question> 질문에 가장 적절한 답변을 작성하세요. 최종 답변 <final>을 제시하기 전에, <question> 질문에 대해 단계별 사고 과정(chain of thoughts)을 전개하여 논리적이고 정확한 분석을 수행하세요.\n",
      "    \n",
      "    <question>\n",
      "    서울의 유명한 관광 코스를 만들어줄래?\n",
      "    </question>\n",
      "    ### 주의사항:\n",
      "    - 불필요한 인사말이나 서두, input은 생략하고, 바로 <response> 부터 작성해주세요.\n",
      "    - 질문과 답변을 반복하지 마세요\n",
      "    - 단계별 사고 과정은 충분히 상세하게 작성하되, 최종 답변은 간결하게 정리하세요\n",
      "    \n",
      "\n",
      "    \n",
      "    ### 응답 형식:\n",
      "    <think>\n",
      "        ### THINKING\n",
      "        [여기에 한국어로 단계별 사고 과정을 상세히 기술하세요. 문제를 분석하고, 가능한 접근법을 검토하며, 논리적 추론을 통해 결론에 도달하는 과정을 보여주세요.]\n",
      "    </think>\n",
      "    <final>\n",
      "        ### FINAL-ANSWER\n",
      "        [THINKING에서 도출된 결론을 간결하고 명확하게 요약하여 한국어로 최종 답변으로 제시하세요.]\n",
      "    </final>\n",
      "\n",
      "    아래 답변입니다.\n",
      "    <think>\n",
      "    \n",
      "        ### THINKING\n",
      "        이 질문에 답하기 위해서는 서울의 주요 관광지와 역사, 문화, 현대적인 랜드마크 등을 균형 있게 포함해야 한다. 명소, 전통 문화 관련 장소, 현대적인 관광지, 그리고 코스 내에서의 이동 편의성을 고려해야 한다. 각 관광지의 특징을 간략히 설명하면서, 서울 관광의 대표성을 갖춘 코스를 구성하는 것이 좋을 것이다.\n",
      "    </think>\n",
      "    <final>\n",
      "        ### FINAL-ANSWER\n",
      "        서울은 한국의 수도로서 다양한 관광 명소가 있습니다. 서울 관광을 즐기기 위한 인기 있는 코스를 다음과 같이 구성할 수 있습니다:\n",
      "\n",
      "## 서울 관광 코스\n",
      "\n",
      "### 역사와 전통의 향연\n",
      "\n",
      "- **경복궁**: 조선시대 최초의 궁궐로, UNESCO 세계유산으로 인정받습니다.\n",
      "- **북촌 한옥마을**: 한복을 입고 산책할 수 있는 전통 한옥이 있는 마을입니다.\n",
      "- **인사동**: 전통 한복 점포와 전통 문화 경험을 할 수 있는 곳입니다.\n",
      "\n",
      "### 현대적인 서울의 매력을 만나보세요\n",
      "\n",
      "- **동대문디자인플라자 (DDP)**: 독특한 구조로 유명한 건축물로, 다양한 전시와 이벤트를 개최합니다.\n",
      "- **여의도**: 금융 중심지이자 관광지로, 독특한 건축물과 랜드마크가 있습니다.\n",
      "- **한강공원**: 한강변에 위치한 공원으로, 자전거 타기, 피크닉, 그리고 경관을 감상할 수 있습니다.\n",
      "\n",
      "### 간단한 이동 방법\n",
      "\n",
      "- **버스**: 서울의 관광지 대부분은 버스로 연결할 수 있습니다.\n",
      "- **도보**: 중심지역에서는 도보로 이동하는 것이 가능합니다.\n",
      "- **Taxi**: 간단한 이동이나 오후에 즐길 수 있습니다.\n",
      "\n",
      "이 코스는 서울의 역사, 전통, 그리고 현대적인 매력을 모두 경험할 수 있도록 설계되었습니다. 각 관광지에서의 소요 시간은 대략 다음과 같습니다:\n",
      "\n",
      "- 경복궁: 1-2시간\n",
      "- 북촌 한옥마을: 2-3시간\n",
      "- 인사동: 1-2시간\n",
      "- 동대문디자인플라자: 1-2시간\n",
      "- 한강공원: 2-3시간\n",
      "\n",
      "이제 서울의 관광을 즐길 시간이 되었습니다!\n",
      "    </final>\n",
      "\n",
      "CPU times: user 35.4 s, sys: 282 ms, total: 35.7 s\n",
      "Wall time: 37.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "max_new_tokens = 1024\n",
    "\n",
    "input_ids = tokenizer(\n",
    "    [inference_prompt_style.format(\"서울의 유명한 관광 코스를 만들어줄래?\") + tokenizer.eos_token], return_tensors=\"pt\"\n",
    ").input_ids\n",
    "\n",
    "input_ids = input_ids.to(device)\n",
    "\n",
    "outputs = peft_model.generate(input_ids, max_new_tokens=max_new_tokens)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-us-west-2-322537213286/checkpoint/Qwen/Qwen3-4B/compressed_model'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.makedirs('shell', exist_ok=True)\n",
    "compressed_model_path='/'.join(checkpoint_s3uri.split(\"/\")[:-1]) + \"/compressed_model\"\n",
    "compressed_model_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning 모델 압축 (model.tar.gz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting shell/finetuned_model_compression_upload.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile shell/finetuned_model_compression_upload.sh\n",
    "\n",
    "cd merged_model\n",
    "cp -r ../src/requirements.txt ./\n",
    "sudo rm -rf code\n",
    "tar cvf - * | pigz > model.tar.gz\n",
    "\n",
    "cd ..\n",
    "mv merged_model/model.tar.gz ./model.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "added_tokens.json\n",
      "config.json\n",
      "generation_config.json\n",
      "merges.txt\n",
      "model-00001-of-00005.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model-00002-of-00005.safetensors\n",
      "model-00003-of-00005.safetensors\n",
      "model-00004-of-00005.safetensors\n",
      "model-00005-of-00005.safetensors\n",
      "model.safetensors.index.json\n",
      "requirements.txt\n",
      "special_tokens_map.json\n",
      "tokenizer_config.json\n",
      "tokenizer.json\n",
      "vocab.json\n",
      "CPU times: user 266 ms, sys: 292 ms, total: 558 ms\n",
      "Wall time: 42.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!sh ./shell/finetuned_model_compression_upload.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: ./model.tar.gz to s3://sagemaker-us-west-2-322537213286/checkpoint/Qwen/Qwen3-4B/compressed_model/finetuned/model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp ./model.tar.gz $compressed_model_path/finetuned/model.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pretrained 모델 압축"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!rm -rf $registered_model/original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting shell/pretrained_model_compression_upload.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile shell/pretrained_model_compression_upload.sh\n",
    "\n",
    "cd qwen3-4b\n",
    "tar cvf - * | pigz > pretrained_model.tar.gz\n",
    "\n",
    "cd ..\n",
    "mv qwen3-4b/pretrained_model.tar.gz ./pretrained_model.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "added_tokens.json\n",
      "config.json\n",
      "generation_config.json\n",
      "merges.txt\n",
      "model-00001-of-00003.safetensors\n",
      "model-00002-of-00003.safetensors\n",
      "model-00003-of-00003.safetensors\n",
      "model.safetensors.index.json\n",
      "README.md\n",
      "special_tokens_map.json\n",
      "tokenizer_config.json\n",
      "tokenizer.json\n",
      "vocab.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: ./pretrained_model.tar.gz to s3://sagemaker-us-west-2-322537213286/checkpoint/Qwen/Qwen3-4B/compressed_model/pretrained/model.tar.gz\n",
      "CPU times: user 665 ms, sys: 881 ms, total: 1.55 s\n",
      "Wall time: 1min 9s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!sh ./shell/pretrained_model_compression_upload.sh\n",
    "!aws s3 cp ./pretrained_model.tar.gz $compressed_model_path/pretrained/model.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'merged_save_dir' (str)\n",
      "Stored 'checkpoint_s3uri' (str)\n",
      "Stored 'compressed_model_path' (str)\n"
     ]
    }
   ],
   "source": [
    "%store merged_save_dir\n",
    "%store checkpoint_s3uri\n",
    "%store compressed_model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "vscode": {
   "interpreter": {
    "hash": "2d58e898dde0263bc564c6968b04150abacfd33eed9b19aaa8e45c040360e146"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
